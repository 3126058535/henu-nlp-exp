{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import log10\n",
    "class Bigram():\n",
    "    #（1）定义函数__init__，用于初始化Bigram类。\n",
    "    #在函数中，初始化词典、词频统计、标点符号等变量。核心代码如下：\n",
    "    def __init__(self, punc, wordset, bigram_wordset):\n",
    "      self.DICT = wordset   # 词典\n",
    "      self.BIGRAM = bigram_wordset     # 词与相邻词的频率统计\n",
    "      self.PUNC = punc     # 标点符号\n",
    "      self.alpha = 2e-4      # 用于未登录词\n",
    "    #（2）定义函数_forwardSplitSentence，用于获取sentence所有可能的切分方案，是实现本分词算法的核心功能函数之一。在函数中，变量sentence表示待切分文本，word_max_len代表最大词长，split_groups用于存储所有方案，最终要返回切分方案列表。核心代码如下：\n",
    "    # 用于获取sentence所有可能的切分方案\n",
    "    def _forwardSplitSentence(self, sentence, word_max_len=5):\n",
    "      #'''\n",
    "      #前向切分\n",
    "      #:param sentence:待切分文本\n",
    "      #:param word_max_len:最大词长\n",
    "      #:return:切分方案列表\n",
    "      #'''\n",
    "      # 所有可能的切分方案\n",
    "        split_groups = []  # 用于存储所有方案\n",
    "        sentence = sentence.strip()  # 去除空格\n",
    "        sentence_len = len(sentence)\n",
    "        if sentence_len < 2:       # sentence只有一个词时，不用切分\n",
    "            return [[sentence]]\n",
    "        # 取待划分句子长度和最大词长度之中的较小值\n",
    "        range_len = [sentence_len,word_max_len][sentence_len > word_max_len]\n",
    "        current_groups = []    # 保存当前二切分结果\n",
    "        single_cut = True     # 是否需要从第一个字后进行切分\n",
    "        for i in range(1, range_len)[::-1]: # 反向取值，i依次取range_len-1,range_len-2,...,1\n",
    "            #子词串在词典中存在，进行二分切分\n",
    "            if self.DICT.__contains__(sentence[:i]) and i != 1: # 逆向切分，不从第一个字后切分\n",
    "                current_groups.append([sentence[:i], sentence[i:]])\n",
    "                single_cut = False\n",
    "        # 没有在字典词组中匹配到，或者第2个字和第3个字构成词\n",
    "        if single_cut or self.DICT.__contains__(sentence[1:3]):\n",
    "            current_groups.append([sentence[:1], sentence[1:]])  # 从第1个字后切分\n",
    "        # 词长为2时，为未登录词的概率较大，保留“为词”的可能性\n",
    "        if sentence_len == 2:\n",
    "            current_groups.append([sentence])\n",
    "        # 对每一个切分，递归组合\n",
    "        for one_group in current_groups:  # one_group为一种二划分结果\n",
    "            if len(one_group) == 1:      # 划分集合中只有一个词，无需再划分\n",
    "                split_groups.append(one_group)\n",
    "                continue\n",
    "            # 对二划分的后一个分片进行再次划分，得到所有划分方案\n",
    "            for child_group in self._forwardSplitSentence(one_group[1]):\n",
    "                child_group.insert(0, one_group[0])  # 在方案前添加二划分的前一个分片\n",
    "                split_groups.append(child_group)    # 加入到结果集\n",
    "        return split_groups\n",
    "    #（3）定义函数getPValue，用于查询二元概率，是实现本分词算法的另一核心功能函数。\n",
    "    #在函数中，front_word为前向词，word是当前词，返回 P(Wi|Wi-1) 。核心代码如下：\n",
    "    def getPValue(self, front_word, word):\n",
    "      #'''\n",
    "      #查询二元概率\n",
    "      #:param front_word: 前向词\n",
    "      #:param word: 当前词\n",
    "      #:return: P(Wi|Wi-1)\n",
    "      #'''\n",
    "        if front_word in self.BIGRAM and word in self.BIGRAM[front_word]:\n",
    "             return self.BIGRAM[front_word][word]\n",
    "        return self.alpha\n",
    "    #（4）定义函数_maxP，用于计算最大概率的切分组合。\n",
    "    #在函数中，变量sentence表示待切分句子，word_max_len为最大不切分词长，返回最优切分方案。核心代码如下：\n",
    "    def _maxP(self, sentence, word_max_len=5):\n",
    "       # 获取切分组合\n",
    "        split_words_group =self._forwardSplitSentence(sentence, word_max_len=word_max_len)\n",
    "        max_p = -99999\n",
    "        best_split = []   # 存放结果\n",
    "        value_dict = {}  # 存放已经计算过概率的子序列\n",
    "        value_dict[u'<start>'] = dict()  # u'<start>'是句子第一个词的前向词\n",
    "        for split_words in split_words_group[::-1]: # 取方案\n",
    "            words_p = 0  # 记录概率\n",
    "            try:\n",
    "                for i in range(len(split_words)):\n",
    "                    word = split_words[i]\n",
    "                    if i == 0:    # 第一个词，特殊处理\n",
    "                        if word not in value_dict[u'<start>']:\n",
    "                            # 获取该词在（前向词|词）中的概率\n",
    "                            value_dict[u'<start>'][word] =log10(self.getPValue(u'<start>', word))\n",
    "                        words_p += value_dict[u'<start>'][word]  # 概率累计\n",
    "                        continue\n",
    "                    front_word = split_words[i - 1]  # 找到前向词\n",
    "                    if front_word not in value_dict:  # 前向词不在字典中\n",
    "                        value_dict[front_word] = dict()  # 将前向词插入\n",
    "                    if word not in value_dict[front_word]:  # 前向词中没有当前词的概率\n",
    "                        value_dict[front_word][word] = log10(self.getPValue(front_word, word))# 赋值\n",
    "                    words_p += value_dict[front_word][word]   # 每个p(wi|wi-1)求和#\n",
    "                    #if words_p < max_p\n",
    "                    #    break\n",
    "            except ValueError:\n",
    "                print(\"Failed to calc:\\:late maxP.\")\n",
    "            if words_p > max_p:     # 获取累加概率最高的划分方案\n",
    "                max_p = words_p\n",
    "                best_split = split_words\n",
    "        return best_split\n",
    "    #（5）定义函数segment，是分词的调用入口。相比英文，中文也具有天然的分割词，即各种标点符号，本函数将输入文本按照标点切分成多个文本，再对其分别进行分词。\n",
    "    #在函数中，定义变量sentence来指定出待分词文本，return返回最终的切分序列。核心代码如下：\n",
    "    def segment(self, sentence):\n",
    "    #'''\n",
    "    #分词调用入口\n",
    "    #:param sentence:待切分句子\n",
    "    #:return:切分词序列\n",
    "    #'''\n",
    "        words = []\n",
    "        sentences = []\n",
    "        # 若含有标点，以标点分割\n",
    "        start = -1\n",
    "        for i in range(len(sentence)):\n",
    "            if sentence[i] in self.PUNC:\n",
    "                sentences.append(sentence[start + 1:i])\n",
    "                sentences.append(sentence[i])\n",
    "                start = i\n",
    "        if not sentences:  # 不含标点\n",
    "            sentences.append(sentence)\n",
    "        for sent in sentences:\n",
    "            words.extend(self._maxP(sent))\n",
    "        return words\n",
    "#（6）定义函数getPunciton，用于获取标点。\n",
    "#在函数中，定义变量file_name来指定文本路径，is_save表示是否保存，save_file表示保存路径。核心代码如下：\n",
    "def getPunciton(file_name='199801.txt', is_save=True, save_file='punction.txt'):\n",
    "    #'''\n",
    "    #获取标点\n",
    "    #:param file_name: 文本路径\n",
    "    #:param is_save: 是否保存\n",
    "    #:param save_file: 保存路径\n",
    "    #:return:\n",
    "    #'''\n",
    "        punction = set(['[', ']'])\n",
    "        with open(file_name, 'rb') as f:\n",
    "            for line in f:\n",
    "                content = line.decode('gbk').strip().split()\n",
    "                # 去掉第一个词“19980101-01-001-001/m”\n",
    "                for word in content[1:]:\n",
    "                    if word.split(u'/')[1] == u'w':\n",
    "                        punction.add(word.split(u'/')[0])\n",
    "        if is_save:\n",
    "            # punction\n",
    "            with open(save_file,\"w\",encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(punction))\n",
    "        return punction\n",
    "#（8）定义函数toWordSet，用于获取词典。\n",
    "#在函数中，定义变量file_name来指定文本路径，is_save表示是否保存，save_file表示保存路径，return返回词典。核心代码如下：\n",
    "def toWordSet(file_name='199801.txt', is_save=True, save_file='wordSet.json'):\n",
    "    #'''\n",
    "    #获取词典\n",
    "    #:param file_name: 文本路径\n",
    "    #:param is_save: 是否保存\n",
    "    #:param save_file: 保存路径\n",
    "    #:return:\n",
    "    #'''\n",
    "        word_dict = {}\n",
    "        with open(file_name, 'rb') as f:\n",
    "            for line in f:\n",
    "                content = line.decode('gbk').strip().split()\n",
    "                # 去掉第一个词“19980101-01-001-001/m”\n",
    "                for word in content[1:]:\n",
    "                    word = word.split(u'/')[0]\n",
    "                    if not word_dict.__contains__(word):\n",
    "                        word_dict[word] = 1\n",
    "                    else:\n",
    "                        word_dict[word] += 1\n",
    "        if is_save:\n",
    "            # 保存wordSet以复用\n",
    "            with open(save_file,'w',encoding='utf-8') as f:\n",
    "                json.dump(word_dict, f, ensure_ascii=False, indent=2)\n",
    "        print(\"successfully get word dictionary!\")\n",
    "        print(\"the total number of words is:{0}\".format(len(word_dict.keys())))\n",
    "        return word_dict\n",
    "#（9）调用函数进行分词\n",
    "#核心代码如下：\n",
    "if __name__ == '__main__':\n",
    "    # 加载符号\n",
    "    punc = getPunciton()\n",
    "    # 加载词典\n",
    "    word_set = json.load(open('wordSet.json','r',encoding='utf-8'))\n",
    "    # 加载Bigram词表\n",
    "    bigram_wordset=json.load(open('word_distri.json','r',encoding='utf-8'))\n",
    "    bigram = Bigram(punc, word_set, bigram_wordset)\n",
    "    s = '我喜欢观赏日出'\n",
    "    print(bigram.segment(s))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
